/**
 * üéØ Service d'Optimisation des Prompts
 * 
 * Optimise les prompts selon le fournisseur d'IA et le contexte
 * pour am√©liorer la qualit√© et la pertinence des r√©ponses.
 */

export interface PromptContext {
  /** Type de requ√™te utilisateur */
  requestType: 'question' | 'task' | 'creative' | 'analysis' | 'conversation';
  /** Complexit√© estim√©e (1-5) */
  complexity: number;
  /** Longueur souhait√©e de la r√©ponse */
  responseLength: 'short' | 'medium' | 'long';
  /** Langue de la requ√™te */
  language: string;
  /** Contexte disponible (RAG, web, etc.) */
  availableContext: {
    hasRagContext: boolean;
    hasWebResults: boolean;
    hasImageContext: boolean;
  };
}

export interface OptimizedPrompt {
  /** Prompt optimis√© pour le fournisseur */
  optimizedPrompt: string;
  /** Param√®tres de g√©n√©ration optimis√©s */
  generationParams: {
    temperature: number;
    topP: number;
    maxTokens: number;
  };
  /** Instructions sp√©cifiques au fournisseur */
  providerInstructions: string[];
  /** Indicateurs de qualit√© attendus */
  qualityIndicators: string[];
}

/**
 * Optimise un prompt pour Gemini
 */
export function optimizePromptForGemini(
  originalPrompt: string,
  context: PromptContext,
  _systemPrompt?: string
): OptimizedPrompt {
  const { requestType, complexity, responseLength, availableContext } = context;

  // Instructions sp√©cifiques √† Gemini
  const geminiInstructions = [
    "Utilise un langage naturel et conversationnel",
    "Structure ta r√©ponse de mani√®re claire et logique",
    "Cite tes sources quand tu en as",
    "Sois pr√©cis et factuel"
  ];

  // Optimisation selon le type de requ√™te
  let optimizedPrompt = originalPrompt;
  let temperature = 0.7;
  let topP = 0.9;
  let maxTokens = getTokenCount(responseLength);

  switch (requestType) {
    case 'question':
      optimizedPrompt = `R√©ponds √† cette question de mani√®re pr√©cise et structur√©e :\n\n${originalPrompt}`;
      temperature = 0.6; // Plus d√©terministe pour les questions
      break;

    case 'task':
      optimizedPrompt = `Ex√©cute cette t√¢che √©tape par √©tape :\n\n${originalPrompt}`;
      temperature = 0.5; // Tr√®s d√©terministe pour les t√¢ches
      maxTokens = Math.floor(maxTokens * 1.5); // Plus de tokens pour les √©tapes
      break;

    case 'creative':
      optimizedPrompt = `Sois cr√©atif et original dans ta r√©ponse √† :\n\n${originalPrompt}`;
      temperature = 0.9; // Plus cr√©atif
      topP = 0.95;
      break;

    case 'analysis':
      optimizedPrompt = `Analyse en profondeur ce sujet :\n\n${originalPrompt}`;
      temperature = 0.7;
      maxTokens = Math.floor(maxTokens * 1.3); // Plus de tokens pour l'analyse
      geminiInstructions.push("Fournis une analyse d√©taill√©e avec des exemples");
      break;

    case 'conversation':
      optimizedPrompt = originalPrompt; // Garde le prompt naturel pour la conversation
      temperature = 0.8; // √âquilibre cr√©ativit√©/d√©terminisme
      break;
  }

  // Ajustements bas√©s sur la complexit√©
  if (complexity >= 4) {
    maxTokens = Math.floor(maxTokens * 1.4);
    geminiInstructions.push("Explique les concepts complexes de mani√®re accessible");
  } else if (complexity <= 2) {
    temperature = Math.max(0.3, temperature - 0.1);
    geminiInstructions.push("Sois concis et direct");
  }

  // Optimisation selon le contexte disponible
  if (availableContext.hasRagContext) {
    optimizedPrompt += "\n\nUtilise les informations du contexte documentaire fourni.";
    geminiInstructions.push("Priorise les informations du contexte documentaire");
  }

  if (availableContext.hasWebResults) {
    optimizedPrompt += "\n\nUtilise les r√©sultats de recherche web r√©cents.";
    geminiInstructions.push("Cite les sources web quand appropri√©");
  }

  if (availableContext.hasImageContext) {
    optimizedPrompt += "\n\nAnalyse √©galement les images fournies.";
    geminiInstructions.push("D√©cris et analyse les images de mani√®re d√©taill√©e");
  }

  // Instructions de qualit√© pour Gemini
  const qualityIndicators = [
    "R√©ponse structur√©e et coh√©rente",
    "Sources cit√©es quand disponibles",
    "Langage naturel et accessible",
    "R√©ponse compl√®te et pertinente"
  ];

  return {
    optimizedPrompt,
    generationParams: {
      temperature,
      topP,
      maxTokens
    },
    providerInstructions: geminiInstructions,
    qualityIndicators
  };
}

/**
 * Optimise un prompt pour OpenAI
 */
export function optimizePromptForOpenAI(
  originalPrompt: string,
  context: PromptContext,
  _systemPrompt?: string
): OptimizedPrompt {
  const { requestType, complexity, responseLength, availableContext } = context;

  // Instructions sp√©cifiques √† OpenAI
  const openaiInstructions = [
    "Utilise un style professionnel et informatif",
    "Structure ta r√©ponse avec des paragraphes clairs",
    "Fournis des exemples concrets quand possible",
    "Sois pr√©cis et d√©taill√©"
  ];

  let optimizedPrompt = originalPrompt;
  let temperature = 0.7;
  let topP = 0.9;
  let maxTokens = getTokenCount(responseLength);

  switch (requestType) {
    case 'question':
      optimizedPrompt = `R√©ponds de mani√®re compl√®te et d√©taill√©e √† cette question :\n\n${originalPrompt}`;
      temperature = 0.6;
      break;

    case 'task':
      optimizedPrompt = `Fournis une solution √©tape par √©tape pour :\n\n${originalPrompt}`;
      temperature = 0.5;
      maxTokens = Math.floor(maxTokens * 1.6);
      break;

    case 'creative':
      optimizedPrompt = `Sois cr√©atif et imaginatif dans ta r√©ponse √† :\n\n${originalPrompt}`;
      temperature = 0.9;
      topP = 0.95;
      break;

    case 'analysis':
      optimizedPrompt = `Effectue une analyse approfondie de :\n\n${originalPrompt}`;
      temperature = 0.7;
      maxTokens = Math.floor(maxTokens * 1.5);
      openaiInstructions.push("Fournis une analyse critique et √©quilibr√©e");
      break;

    case 'conversation':
      optimizedPrompt = originalPrompt;
      temperature = 0.8;
      break;
  }

  // Ajustements pour OpenAI
  if (complexity >= 4) {
    maxTokens = Math.floor(maxTokens * 1.3);
    openaiInstructions.push("Explique les concepts techniques de mani√®re accessible");
  }

  // Gestion du contexte
  if (availableContext.hasRagContext || availableContext.hasWebResults) {
    optimizedPrompt += "\n\nBase-toi sur les informations contextuelles fournies.";
    openaiInstructions.push("Int√®gre les informations contextuelles de mani√®re coh√©rente");
  }

  const qualityIndicators = [
    "R√©ponse bien structur√©e",
    "Exemples et d√©tails pertinents",
    "Style professionnel et clair",
    "R√©ponse compl√®te et approfondie"
  ];

  return {
    optimizedPrompt,
    generationParams: {
      temperature,
      topP,
      maxTokens
    },
    providerInstructions: openaiInstructions,
    qualityIndicators
  };
}

/**
 * Optimise un prompt pour Mistral
 */
export function optimizePromptForMistral(
  originalPrompt: string,
  context: PromptContext,
  _systemPrompt?: string
): OptimizedPrompt {
  const { requestType, complexity, responseLength, availableContext } = context;

  // Instructions sp√©cifiques √† Mistral
  const mistralInstructions = [
    "Utilise un langage clair et direct",
    "Sois concis mais complet",
    "Priorise la pr√©cision",
    "Adapte ton niveau de d√©tail au contexte"
  ];

  let optimizedPrompt = originalPrompt;
  let temperature = 0.7;
  let topP = 0.95;
  let maxTokens = getTokenCount(responseLength);

  switch (requestType) {
    case 'question':
      optimizedPrompt = `R√©ponds pr√©cis√©ment √† cette question :\n\n${originalPrompt}`;
      temperature = 0.6;
      break;

    case 'task':
      optimizedPrompt = `R√©sous cette t√¢che m√©thodiquement :\n\n${originalPrompt}`;
      temperature = 0.5;
      maxTokens = Math.floor(maxTokens * 1.4);
      break;

    case 'creative':
      optimizedPrompt = `Sois cr√©atif et original :\n\n${originalPrompt}`;
      temperature = 0.9;
      topP = 0.95;
      break;

    case 'analysis':
      optimizedPrompt = `Analyse m√©thodiquement ce sujet :\n\n${originalPrompt}`;
      temperature = 0.7;
      maxTokens = Math.floor(maxTokens * 1.3);
      mistralInstructions.push("Fournis une analyse structur√©e et logique");
      break;

    case 'conversation':
      optimizedPrompt = originalPrompt;
      temperature = 0.8;
      break;
  }

  // Optimisations sp√©cifiques √† Mistral
  if (complexity >= 4) {
    maxTokens = Math.floor(maxTokens * 1.2);
    mistralInstructions.push("Explique les concepts complexes de mani√®re progressive");
  } else if (complexity <= 2) {
    temperature = Math.max(0.3, temperature - 0.1);
    mistralInstructions.push("Sois concis et va droit au but");
  }

  // Gestion du contexte pour Mistral
  if (availableContext.hasRagContext) {
    optimizedPrompt += "\n\nUtilise les documents de r√©f√©rence fournis.";
    mistralInstructions.push("R√©f√©rence clairement les sources documentaires");
  }

  if (availableContext.hasWebResults) {
    optimizedPrompt += "\n\nInt√®gre les informations des recherches web.";
    mistralInstructions.push("Cite les sources web de mani√®re appropri√©e");
  }

  const qualityIndicators = [
    "R√©ponse claire et structur√©e",
    "Pr√©cision et exactitude",
    "Int√©gration coh√©rente des sources",
    "R√©ponse adapt√©e au niveau de complexit√©"
  ];

  return {
    optimizedPrompt,
    generationParams: {
      temperature,
      topP,
      maxTokens
    },
    providerInstructions: mistralInstructions,
    qualityIndicators
  };
}

/**
 * Optimise un prompt selon le fournisseur
 */
export function optimizePrompt(
  provider: 'gemini' | 'openai' | 'mistral',
  originalPrompt: string,
  context: PromptContext,
  systemPrompt?: string
): OptimizedPrompt {
  switch (provider) {
    case 'gemini':
      return optimizePromptForGemini(originalPrompt, context, systemPrompt);
    case 'openai':
      return optimizePromptForOpenAI(originalPrompt, context, systemPrompt);
    case 'mistral':
      return optimizePromptForMistral(originalPrompt, context, systemPrompt);
    default:
      throw new Error(`Fournisseur non support√©: ${provider}`);
  }
}

/**
 * Analyse le type de requ√™te et sa complexit√©
 */
export function analyzeRequest(prompt: string): PromptContext {
  const lowerPrompt = prompt.toLowerCase();
  
  // D√©tection du type de requ√™te
  let requestType: PromptContext['requestType'] = 'question';
  
  if (lowerPrompt.includes('√©cris') || lowerPrompt.includes('cr√©e') || 
      lowerPrompt.includes('invente') || lowerPrompt.includes('imagine')) {
    requestType = 'creative';
  } else if (lowerPrompt.includes('fais') || lowerPrompt.includes('r√©sous') ||
             lowerPrompt.includes('calcule') || lowerPrompt.includes('r√©sout')) {
    requestType = 'task';
  } else if (lowerPrompt.includes('analyse') || lowerPrompt.includes('explique') ||
             lowerPrompt.includes('compare') || lowerPrompt.includes('√©value')) {
    requestType = 'analysis';
  } else if (lowerPrompt.includes('salut') || lowerPrompt.includes('bonjour') ||
             lowerPrompt.includes('comment √ßa va') || lowerPrompt.includes('merci')) {
    requestType = 'conversation';
  }

  // Estimation de la complexit√© (1-5)
  let complexity = 3; // Complexit√© moyenne par d√©faut
  
  const wordCount = prompt.split(/\s+/).length;
  const hasTechnicalTerms = /[A-Z]{2,}|[0-9]+%|[0-9]+‚Ç¨|api|http|sql|json|xml/i.test(prompt);
  const hasMultipleQuestions = (prompt.match(/\?/g) || []).length > 1;
  const hasLongExplanation = wordCount > 50;
  
  if (hasTechnicalTerms) complexity += 1;
  if (hasMultipleQuestions) complexity += 1;
  if (hasLongExplanation) complexity += 1;
  if (wordCount < 10) complexity -= 1;
  
  complexity = Math.max(1, Math.min(5, complexity));

  // Estimation de la longueur souhait√©e
  let responseLength: PromptContext['responseLength'] = 'medium';
  
  if (lowerPrompt.includes('court') || lowerPrompt.includes('bref') || lowerPrompt.includes('r√©sum√©')) {
    responseLength = 'short';
  } else if (lowerPrompt.includes('d√©taill√©') || lowerPrompt.includes('complet') || 
             lowerPrompt.includes('exhaustif') || lowerPrompt.includes('approfondi')) {
    responseLength = 'long';
  }

  // D√©tection de la langue
  const language = detectLanguage(prompt);

  return {
    requestType,
    complexity,
    responseLength,
    language,
    availableContext: {
      hasRagContext: false, // Sera d√©fini par l'appelant
      hasWebResults: false,
      hasImageContext: false
    }
  };
}

/**
 * D√©tecte la langue du prompt
 */
function detectLanguage(prompt: string): string {
  // D√©tection simple bas√©e sur les mots-cl√©s
  const frenchWords = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'avec', 'pour', 'dans', 'sur'];
  const englishWords = ['the', 'a', 'an', 'and', 'or', 'with', 'for', 'in', 'on', 'is', 'are', 'was', 'were'];
  
  const words = prompt.toLowerCase().split(/\s+/);
  const frenchCount = words.filter(word => frenchWords.includes(word)).length;
  const englishCount = words.filter(word => englishWords.includes(word)).length;
  
  if (frenchCount > englishCount) return 'fr';
  if (englishCount > frenchCount) return 'en';
  return 'fr'; // Par d√©faut fran√ßais
}

/**
 * Calcule le nombre de tokens recommand√© selon la longueur souhait√©e
 */
function getTokenCount(responseLength: PromptContext['responseLength']): number {
  switch (responseLength) {
    case 'short': return 512;
    case 'medium': return 1024;
    case 'long': return 2048;
    default: return 1024;
  }
}

/**
 * G√©n√®re des prompts de suivi optimis√©s
 */
export function generateFollowUpPrompts(
  _originalResponse: string,
  provider: 'gemini' | 'openai' | 'mistral',
  _context: PromptContext
): string[] {
  const followUps: string[] = [];
  
  // Prompts g√©n√©riques
  followUps.push("Peux-tu d√©velopper ce point ?");
  followUps.push("As-tu des exemples concrets ?");
  followUps.push("Peux-tu expliquer plus simplement ?");
  
  // Prompts sp√©cifiques au fournisseur
  switch (provider) {
    case 'gemini':
      followUps.push("Peux-tu me donner plus de d√©tails sur ce sujet ?");
      followUps.push("As-tu d'autres perspectives √† partager ?");
      break;
    case 'openai':
      followUps.push("Peux-tu analyser les implications de cela ?");
      followUps.push("Quelles sont les meilleures pratiques dans ce domaine ?");
      break;
    case 'mistral':
      followUps.push("Peux-tu r√©sumer les points cl√©s ?");
      followUps.push("Quels sont les aspects les plus importants ?");
      break;
  }
  
  return followUps;
}
